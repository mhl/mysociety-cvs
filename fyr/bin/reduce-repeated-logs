#!/usr/bin/perl -w
#
# reduce-repeated-logs:
# Finds messages with lots of duplicated log lines. Merges the adjacent
# duplicates into one log line, with number of repetitions after it.
#
# e.g.
# unexpected error (type RABX::Error::User) while processing message: Representative ID '13531' not found (repeated 16138 times)
#
# Copyright (c) 2006 UK Citizens Online Democracy. All rights reserved.
# Email: francis@mysociety.org; WWW: http://www.mysociety.org/
#

# TODO:
# Cope with messages which have alternating lines repeated many times.

my $rcsid = ''; $rcsid .= '$Id: reduce-repeated-logs,v 1.3 2006-08-12 15:09:50 francis Exp $';

use strict;
require 5.8.0;

# Number of lines which must exactly repeat within a message for this script to
# look for adjacent duplicates within that message.
my $low = 1000;

# Horrible boilerplate to set up appropriate library paths.
use FindBin;
use lib "$FindBin::Bin/../perllib";
use lib "$FindBin::Bin/../../perllib";

use Data::Dumper;
use POSIX qw(strftime);

use mySociety::Config;
BEGIN {
    mySociety::Config::set_file("$FindBin::Bin/../conf/general");
}
use mySociety::DBHandle qw(dbh);
#DBI->trace(1);
use FYR;

# Takes an array of ids of duplicate message_log items, and an extra count to
# add to this. Deletes all of the message except the last one, and appends
# total count to that.
sub merge_messages {
    my ($dups, $c_extra) = @_;
    my @dups = @$dups;
    my $c = scalar(@dups) + $c_extra;
    my $last = pop @dups;
    #print Dumper(\@dups);
    #print "count: " . $c . "\n";
    #print "last: " . $last . "\n";
    while (scalar(@dups) > 5000) {
        my @start = @dups[0..5000];
        @dups = @dups[5001..$#dups];
        dbh()->do("delete from message_log where order_id in (". join(",", @start) . ")");
    }
    dbh()->do("delete from message_log where order_id in (". join(",", @dups) . ")");
    dbh()->do("update message_log set message = 
            (replace(message, coalesce(substring(message from ' \\\\(repeated .+ times\\\\)\$'), ''),'')
            || ' (repeated $c times)') where order_id = ?", {}, $last);
    dbh()->commit();
}
 
# Takes an array of ids of alternating duplicate message_log items, and
# merges them together, much like merge_messages for non-alternating repeats.
sub merge_messages_pairs {
    my ($dups, $c_extra) = @_;
    my @dups = @$dups;
    my $c = (scalar(@dups) + $c_extra) / 2;
    my $last1 = pop @dups;
    my $last2 = pop @dups;
#    print Dumper(\@dups);
#    print "count: $c\n";
#    print "last1: $last1 last2: $last2\n";
#    exit;
    while (scalar(@dups) > 5000) {
        my @start = @dups[0..5000];
        @dups = @dups[5001..$#dups];
        dbh()->do("delete from message_log where order_id in (". join(",", @start) . ")");
    }
    dbh()->do("delete from message_log where order_id in (". join(",", @dups) . ")");
    dbh()->do("update message_log set message = 
            (replace(message, coalesce(substring(message from ' \\\\(repeated .+ times\\\\)\$'), ''),'')
            || ' (repeated in pair about $c times)') where order_id = ?", {}, $last1);
    dbh()->do("update message_log set message = 
            (replace(message, coalesce(substring(message from ' \\\\(repeated .+ times\\\\)\$'), ''),'')
            || ' (repeated in pair about $c times)') where order_id = ?", {}, $last2);
    dbh()->commit();
}

# Takes a message id, and merges adjacent duplicate message_log items for that message.
sub reduce_log($) {
    my ($message_id) = @_;
    my $stm = dbh()->prepare("
        select order_id, exceptional, whenlogged, state, message, editor
        from message_log where message_id = ? order by order_id");
    $stm->execute($message_id);
    my ($p_exceptional, $p_whenlogged, $p_state, $p_message, $p_editor);
    my $p_p_message;
    my @dups;
    my $c_extra = 0;
    my $matching_pairs = 0;
    while (my ($order_id, $exceptional, $whenlogged, $state, $message, $editor) = $stm->fetchrow_array()) {
        $editor = '' if (!$editor);
        if ($message =~ m/^(.+) \(repeated (?:in pair about )?(\d+) times\)$/) {
            $message = $1;
            my $c_message = $2 - 1;
            $c_extra += ($c_message - 1);
        }
        #print scalar(@dups) . " $order_id $exceptional $whenlogged $state $message $editor\n";

        # See if meta data same as last message
        my $got = 0;
        if (!$p_state || 
            (   $p_exceptional eq $exceptional && 
                $p_whenlogged <= $whenlogged &&
                $p_state eq $state && 
                $p_editor eq $editor)
            ) {
                if (!$p_state || (!$matching_pairs && $p_message eq $message)) {
                    # Either at beginning, or message same as last one
                    push @dups, $order_id;
                    $got = 1;
                } elsif (!$matching_pairs && scalar(@dups) == 1) {
                    # Start looking for pair matches
                    $matching_pairs = 1;
                    push @dups, $order_id;
                    $got = 1;
                } elsif ($matching_pairs && $p_p_message eq $message) {
                    # Message same as one two ago
                    push @dups, $order_id;
                    $got = 1;
                }
        } 
        if (!$got) {
            # If we got more than one row, then merge in 
            if (!$matching_pairs && scalar(@dups) >= 2) {
                merge_messages(\@dups, $c_extra);
            } elsif ($matching_pairs && scalar(@dups) >= 8) { # higher tolerance for pairs
                merge_messages_pairs(\@dups, $c_extra);
            }
            # Reset
            @dups = ($order_id);
            $c_extra = 0;
            $matching_pairs = 0;
        }
        $p_p_message = $p_message;
        $p_exceptional = $exceptional;
        $p_whenlogged = $whenlogged;
        $p_state = $state;
        $p_message = $message;
        $p_editor = $editor;
    }
    if (!$matching_pairs && scalar(@dups) >= 2) {
        merge_messages(\@dups, $c_extra);
    } elsif ($matching_pairs && scalar(@dups) >= 8) { # higher tolerance for pairs
        merge_messages_pairs(\@dups, $c_extra);
    }
}

reduce_log('7f715e54e05dd8428f8a');
exit;

# Read everything with lots of duplicates, using heuristic here
my $st = dbh()->prepare("select count(*) as c, message_id, message
    from message_log group by message_id, message having count(*) > $low order by c desc");
$st->execute();
while (my ($count, $message_id, $message) = $st->fetchrow_array()) {
    my ($total_rows) = dbh()->selectrow_array("select count(*) from message_log");
    print "Total rows: $total_rows\n"; 

    # Do proper merging of adjacent items
    print "Reducing $message_id $message...\n";
    reduce_log($message_id);
}

